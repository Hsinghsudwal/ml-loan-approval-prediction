import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler, Normalizer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn. compose import ColumnTransformer
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier, BaggingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from lightgbm import LGBMClassifier
from catboost import CatBoostClassifier
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay, roc_auc_score, classification_report
from sklearn.metrics import roc_curve, auc
from sklearn.metrics import precision_score, recall_score, f1_score 

from imblearn.over_sampling import SMOTE
from sklearn.model_selection import RandomizedSearchCV
import scipy 
from scipy import stats
from scipy.stats import uniform, randint



data = pd.read_csv(r'../data/loan_approval_dataset.csv')
data.head()





data.info()





data.columns





data.isnull().sum()


data.describe().T


data.duplicated().sum()


# remove spaces from column names
data.columns = data.columns.str.strip()
data.columns


numerical_columns=[]
categorical_columns=[]
for i in data.columns:
    if data[i].dtype == 'object':
        categorical_columns.append(i)
    else:
        numerical_columns.append(i)
    


# Violin plot for numerical columns
for num in numerical_columns:
    sns.violinplot(x='loan_status',y= num, data =data)
    plt.show()


for num in numerical_columns:
    sns.histplot(data[num], label='Train', kde=True)
    plt.show()


for num in numerical_columns:
    sns.boxplot(data=[data[num]])
    plt.show()


for cat in categorical_columns:
  loan_status_counts = data.groupby([cat, 'loan_status'])['loan_status'].count().unstack()
  loan_status_counts.plot(kind='bar', stacked=True, figsize=(10, 6))
  plt.title(f'Chart of {cat} by loan_status')
  plt.xlabel(cat)
  plt.ylabel('Count')
  plt.show()


# skew_value = data.skew().sort_values(ascending=False)
# skew_value
# remove outlier
Q1=data.quantile(0.25)
Q3=data.quantile(0.75)
IQR= Q3-Q1
data=data[~((data < (Q1 - 1.5 * IQR)) |(data > (Q3 + 1.5 * IQR))).any(axis=1)]





# Preprocessing
# Remove outlier data
# OnehotEncoder
# StandardScaler


data.nunique()


data.loan_status.value_counts()


data['self_employed'].unique()


data['loan_status'].unique()


data['loan_status'].isnull().sum()


# label encoding target

data['loan_status']=data['loan_status'].map({' Approved':1, ' Rejected': 0})
# data['self_employed']=data['self_employed'].map({' No':0, ' Yes':1})
data.head()


# Split the data
y=data['loan_status']
x=data.drop(columns=['loan_id','loan_status'],axis = 1)
x


X=x.copy()
Y=y.copy()


numerical_features = X.select_dtypes(include=['int64']).columns.tolist()
categorical_features=X.select_dtypes(include=['object']).columns.tolist()
categorical_features


numerical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='mean')), 
    ('scaler', StandardScaler())  
                ])


categorical_transformer=Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('ohe', OneHotEncoder(handle_unknown='ignore')),
                ])


preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, numerical_features),
        ('cat', categorical_transformer,categorical_features)]
                )


preprocessed = preprocessor.fit_transform(X)



all_features=preprocessor.get_feature_names_out()
all_features


# col=data.columns
df=pd.DataFrame(preprocessed, columns=all_features)
df


# train test: pipeline, simpleImpute, ohe, scaler, 
X_train, X_test, y_train, y_test = train_test_split(df, y, test_size=0.2, random_state=42)
X_train.shape, X_test.shape, y_train.shape, y_test.shape


# Train and evaluate different models


rfc=RandomForestClassifier()
# Fit the model
rfc.fit(X_train, y_train)
# Predict using the best model
y_pred = rfc.predict(X_test)
# Evaluate the model
print('Model: ',rfc)
print(f"Accuracy score: {accuracy_score(y_test, y_pred):.3f}")

print("\nClassification Report:", classification_report(y_test, y_pred))
cm=confusion_matrix(y_test,y_pred)
sns.heatmap(cm, annot=True, fmt='g')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.show()
# Calculate the ROC curve
fpr, tpr, thresholds = roc_curve(y_test, y_pred)

# Calculate the AUC score
auc_score = auc(fpr, tpr)
x1=np.linspace(0,1,100)
# Plot the ROC curve
plt.plot(fpr, tpr, label="ROC curve (AUC = %0.2f)" % auc_score)
plt.plot(x1,x1,label='baseline')
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC curve for binary classification")
plt.legend()
plt.show()


dt=DecisionTreeClassifier()
# Fit the model
dt.fit(X_train, y_train)
# Predict using the best model
y_pred = dt.predict(X_test)
# Evaluate the model
print('Model: ',dt)
print(f"Accuracy score: {accuracy_score(y_test, y_pred):.3f}")
print("\nClassification Report:", classification_report(y_test, y_pred))
cm=confusion_matrix(y_test,y_pred)
sns.heatmap(cm, annot=True, fmt='g')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.show()
# Calculate the ROC curve
fpr, tpr, thresholds = roc_curve(y_test, y_pred)

# Calculate the AUC score
auc_score = auc(fpr, tpr)
x1=np.linspace(0,1,100)
# Plot the ROC curve
plt.plot(fpr, tpr, label="ROC curve (AUC = %0.2f)" % auc_score)
plt.plot(x1,x1,label='baseline')
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC curve for binary classification")
plt.legend()
plt.show()


xgb=XGBClassifier()
# Fit the model
xgb.fit(X_train, y_train)
# Predict using the best model
y_pred = xgb.predict(X_test)
# Evaluate the model
print('Model: ',xgb)
print(f"Accuracy score: {accuracy_score(y_test, y_pred):.3f}")
print("\nClassification Report:", classification_report(y_test, y_pred))
cm=confusion_matrix(y_test,y_pred)
sns.heatmap(cm, annot=True, fmt='g')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.show()
# Calculate the ROC curve
fpr, tpr, thresholds = roc_curve(y_test, y_pred)

# Calculate the AUC score
auc_score = auc(fpr, tpr)
x1=np.linspace(0,1,100)
# Plot the ROC curve
plt.plot(fpr, tpr, label="ROC curve (AUC = %0.2f)" % auc_score)
plt.plot(x1,x1,label='baseline')
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC curve for binary classification")
plt.legend()
plt.show()


bgc=BaggingClassifier()
# Fit the model
bgc.fit(X_train, y_train)
# Predict using the best model
y_pred = bgc.predict(X_test)
# Evaluate the model
print('Model: ',bgc)
print(f"Accuracy score: {accuracy_score(y_test, y_pred):.3f}")
print("\nClassification Report:", classification_report(y_test, y_pred))
cm=confusion_matrix(y_test,y_pred)
sns.heatmap(cm, annot=True, fmt='g')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.show()
# Calculate the ROC curve
fpr, tpr, thresholds = roc_curve(y_test, y_pred)

# Calculate the AUC score
auc_score = auc(fpr, tpr)
x1=np.linspace(0,1,100)
# Plot the ROC curve
plt.plot(fpr, tpr, label="ROC curve (AUC = %0.2f)" % auc_score)
plt.plot(x1,x1,label='baseline')
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC curve for binary classification")
plt.legend()
plt.show()


def train_and_evaluate(model, X_train, X_test, y_train, y_test, model_name):

  model.fit(X_train, y_train)
  y_pred = model.predict(X_test)

  # Evaluate the model
  print(f"\n{model_name} Model:")
  accuracy=accuracy_score(y_test, y_pred)
  print(f"Accuracy score:, {accuracy_score(y_test, y_pred):.3f}")
  roc_auc = roc_auc_score(y_test, y_pred)
  class_report = classification_report(y_test, y_pred)
    
  # Visualize confusion matrix
  cm=confusion_matrix(y_test,y_pred)
  sns.heatmap(cm, annot=True, fmt='g')
  plt.xlabel('Predicted Labels')
  plt.ylabel('True Labels')
  plt.show()
  print(f"ROC AUC: {roc_auc}")
  print(f"Classification Report:\n{class_report}")

  # Visualize ROC AUC
  y_pred_proba = model.predict_proba(X_test)[:, 1]
  fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)
  roc_auc = auc(fpr, tpr)
  plt.figure()
  plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
  plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
  plt.xlim([0.0, 1.0])
  plt.ylim([0.0, 1.05])
  plt.xlabel('False Positive Rate')
  plt.ylabel('True Positive Rate')
  plt.title(f'ROC AUC for {model_name} Model')
  plt.legend(loc="lower right")
  plt.show()

  return accuracy, roc_auc
    


# Train and evaluate different models

models = {
    "Random Forest": RandomForestClassifier(),
    "Decision Tree": DecisionTreeClassifier(),
    "XGBoost": XGBClassifier(),
    "Bagging Classifier": BaggingClassifier(),
    "LightGBM": LGBMClassifier(),
    "CatBoost": CatBoostClassifier(),
    
}


results = {}

for model_name, model in models.items():
  accuracy, roc_auc = train_and_evaluate(model, X_train, X_test, y_train, y_test, model_name)
  results[model_name] = {"accuracy": accuracy, "roc_auc": roc_auc}


# Initialize the XGBClassifier
xgb_classifier = XGBClassifier(random_state=42)

# Define the hyperparameter search space
param_dist = {
    'n_estimators': randint(50, 500),
    'learning_rate': uniform(0.01, 0.3),
    'max_depth': randint(2, 10),
    'min_child_weight': randint(1, 10),
    'gamma': uniform(0, 5),
    'subsample': uniform(0.6, 0.4),
    'colsample_bytree': uniform(0.6, 0.4),
    'reg_alpha': uniform(0, 1),
    'reg_lambda': uniform(0, 1),
}

# Set up RandomizedSearchCV
random_search = RandomizedSearchCV(
    estimator=xgb_classifier,
    param_distributions=param_dist,
    n_iter=50,
    scoring='accuracy',
    cv=5,
    random_state=42,
    n_jobs=-1
)

# Fit the model
random_search.fit(X_train, y_train)

# Best estimator
best_model = random_search.best_estimator_
print(f"Best parameters found: {random_search.best_params_}")

# Predict using the best model
y_pred = best_model.predict(X_test)

# Evaluate the model
print(f"Accuracy score: {accuracy_score(y_test, y_pred):.3f}")
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

cm=confusion_matrix(y_test,y_pred)
sns.heatmap(cm, annot=True, fmt='g')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.show()

# Calculate the ROC curve
fpr, tpr, thresholds = roc_curve(y_test, y_pred)

# Calculate the AUC score
auc_score = auc(fpr, tpr)
x1=np.linspace(0,1,100)
# Plot the ROC curve
plt.plot(fpr, tpr, label="ROC curve (AUC = %0.2f)" % auc_score)
plt.plot(x1,x1,label='baseline')
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC curve for binary classification")
plt.legend()
plt.show()    


# see improve with smote
sm=SMOTE()
x_sm, y_sm =sm.fit_resample(df, y)
# x_sm
# y.ravel()


X_train_sm, X_test_sm, y_train_sm, y_test_sm = train_test_split(x_sm, y_sm, test_size=0.2, random_state=42)
X_train_sm.shape, X_test_sm.shape, y_train_sm.shape, y_test_sm.shape


models_v1 = {
    "Random Forest": RandomForestClassifier(),
    "Decision Tree": DecisionTreeClassifier(),
    "XGBoost": XGBClassifier(),
}


results_v1 = {}

for model_name, model in models_v1.items():
  accuracy, roc_auc = train_and_evaluate(model, X_train_sm, X_test_sm, y_train_sm, y_test_sm, model_name)
  results_v1[model_name] = {"accuracy": accuracy, "roc_auc": roc_auc}


# Initialize the XGBClassifier
xgb_classifier = XGBClassifier(random_state=42)

# Define the hyperparameter search space
param_dist = {
    'n_estimators': randint(50, 500),
    'learning_rate': uniform(0.01, 0.3),
    'max_depth': randint(2, 10),
    'min_child_weight': randint(1, 10),
    'gamma': uniform(0, 5),
    'subsample': uniform(0.6, 0.4),
    'colsample_bytree': uniform(0.6, 0.4),
    'reg_alpha': uniform(0, 1),
    'reg_lambda': uniform(0, 1),
}

# Set up RandomizedSearchCV
random_search = RandomizedSearchCV(
    estimator=xgb_classifier,
    param_distributions=param_dist,
    n_iter=50,
    scoring='accuracy',
    cv=5,
    random_state=42,
    n_jobs=-1
)

# Fit the model
random_search.fit(X_train_sm, y_train_sm)

# Best estimator
best_model = random_search.best_estimator_
print(f"Best parameters found: {random_search.best_params_}")

# Predict using the best model
y_pred = best_model.predict(X_test_sm)

# Evaluate the model
print(f"Accuracy score: {accuracy_score(y_test_sm, y_pred):.3f}")
print("\nClassification Report:")
print(classification_report(y_test_sm, y_pred))

cm=confusion_matrix(y_test_sm,y_pred)
sns.heatmap(cm, annot=True, fmt='g')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.show()

# Calculate the ROC curve
fpr, tpr, thresholds = roc_curve(y_test_sm, y_pred)

# Calculate the AUC score
auc_score = auc(fpr, tpr)
x1=np.linspace(0,1,100)
# Plot the ROC curve
plt.plot(fpr, tpr, label="ROC curve (AUC = %0.2f)" % auc_score)
plt.plot(x1,x1,label='baseline')
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC curve for binary classification")
plt.legend()
plt.show()    


X_train_xx, X_test_xx, y_train_xx, y_test_xx = train_test_split(xx_df, yy, test_size=0.2, random_state=42)
X_train_xx.shape, X_test_xx.shape, y_train_xx.shape, y_test_xx.shape


results_xx = {}

for model_name, model in models_v1.items():
  accuracy, roc_auc = train_and_evaluate(model, X_train_xx, X_test_xx, y_train_xx, y_test_xx, model_name)
  results_xx[model_name] = {"accuracy": accuracy, "roc_auc": roc_auc}






